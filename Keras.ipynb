{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning - Una introducción a Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:19:04.568882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 16:19:04.986193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-31 16:19:04.986244: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-31 16:19:05.079842: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.protobuf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(seed)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/context.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/core/framework/function_pb2.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m _b\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;01mlambda\u001b[39;00m x:x) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28;01mlambda\u001b[39;00m x:x\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m message \u001b[38;5;28;01mas\u001b[39;00m _message\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reflection \u001b[38;5;28;01mas\u001b[39;00m _reflection\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.protobuf'"
     ]
    }
   ],
   "source": [
    "# declarar una seed para que los resultados sean reproducibles\n",
    "seed = 2023\n",
    "\n",
    "# fijar la semilla aleatoria en NumPy y TensorFlow\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import\tos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar el dataset\n",
    "FOLDER = 'datasets/'\n",
    "FILE = 'iris.csv'\n",
    "path = os.path.join(FOLDER, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer el dataset con pandas\n",
    "df = pd.read_csv(path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codificar la variable Species en one-hot\n",
    "df = pd.get_dummies(df, columns=['Species'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separar características y etiquetas\n",
    "x = df.drop(['Species_Iris-setosa', 'Species_Iris-versicolor', 'Species_Iris-virginica'], axis=1)\n",
    "x = x.drop(['Id'], axis=1)\n",
    "y = df[['Species_Iris-setosa', 'Species_Iris-versicolor', 'Species_Iris-virginica']]\n",
    "\n",
    "# Separar los datos en entrenamiento, validación y prueba con una proporción de 60%, 20% y 20% respectivamente\n",
    "# Separar datos de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Separar datos de entrenamiento y validación\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Normalizar características\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_val shape:', x_val.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que he hecho es dividir el conjunto de datos en 3: entrenamiento, validación y prueba. \n",
    "\n",
    "**Conjunto de Entrenamiento:** Es el principal conjunto de datos utilizado para entrenar el modelo. Los algoritmos aprenden a reconocer patrones y hacer predicciones basadas en este conjunto.\n",
    "\n",
    "**Conjunto de Validación o Desarrollo:** Se utiliza para ajustar y optimizar el modelo. Por ejemplo, durante el entrenamiento, puedes usar este conjunto para probar el rendimiento del modelo y ajustar hiperparámetros. Es fundamental para identificar problemas como el sobreajuste.\n",
    "\n",
    "**Conjunto de Prueba:** Sirve para evaluar el rendimiento del modelo después del entrenamiento y la validación. Proporciona una estimación imparcial de cómo el modelo funcionará en datos no vistos.\n",
    "\n",
    "**Proporciones de División:**\n",
    "\n",
    "En la era anterior del aprendizaje profundo, era común dividir los datos en proporciones como 70/30 (entrenamiento/prueba) o 60/20/20 (entrenamiento/validación/prueba). Sin embargo, con la llegada de grandes conjuntos de datos, estas proporciones están cambiando. Por ejemplo, si se cuenta con un millón de ejemplos, es posible que solo el 1% o incluso menos sea usado para validación y prueba, dejando el 98% o más para el entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del modelo en Keras:\n",
    "\n",
    "\n",
    "**1. Clase `Sequential`:**\n",
    "\n",
    "La clase `Sequential` es una forma lineal y sencilla de definir modelos en Keras. Permite apilar capas de manera secuencial, una encima de la otra, para construir la arquitectura del modelo.\n",
    "\n",
    "**Métodos principales:**\n",
    "\n",
    "- **add()**: Añade una capa al modelo.\n",
    "- **compile()**: Configura el proceso de aprendizaje del modelo.\n",
    "- **fit()**: Entrena el modelo con los datos.\n",
    "- **evaluate()**: Evalúa el rendimiento del modelo.\n",
    "- **predict()**: Realiza predicciones con el modelo entrenado.\n",
    "\n",
    "**Uso básico:**\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(10,)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Función `add`:**\n",
    "\n",
    "La función `add()` pertenece a la clase `Sequential` y se utiliza para agregar una capa a la arquitectura del modelo. Las capas se añaden en el orden en que se invoca esta función. Por ejemplo:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "En el código anterior, estamos agregando tres capas densamente conectadas. La primera capa tiene 32 unidades, la segunda 16 unidades y la última tiene 1 unidad.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Clase `Dense`:**\n",
    "\n",
    "La clase `Dense` representa una capa densamente conectada, también conocida como capa completamente conectada. Es una de las capas más comúnmente utilizadas en redes neuronales.\n",
    "\n",
    "**Parámetros principales:**\n",
    "\n",
    "- **units**: Número de neuronas en la capa. Es un entero positivo.\n",
    "- **activation**: Función de activación a utilizar. Puede ser una cadena con el nombre de la función (`'relu'`, `'sigmoid'`, `'softmax'`, etc.) o una función personalizada.\n",
    "- **input_shape**: Dimensiones de los datos de entrada. Solo es necesario en la primera capa del modelo. Por ejemplo, para datos con 10 características, usaríamos `input_shape=(10,)`.\n",
    "- **kernel_initializer**: Inicializador para los pesos de las conexiones con las neuronas. Por defecto es `'glorot_uniform'`, pero hay muchas otras opciones disponibles.\n",
    "- **bias_initializer**: Inicializador para los sesgos. Por defecto es `'zeros'`.\n",
    "- **kernel_regularizer**: Función regularizadora para los pesos. Puede ser útil para prevenir el sobreajuste.\n",
    "- **bias_regularizer**: Función regularizadora para los sesgos.\n",
    "\n",
    "**Uso básico:**\n",
    "```python\n",
    "Dense(units=32, activation='relu', input_shape=(10,))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "En resumen:\n",
    "\n",
    "- La clase `Sequential` permite crear modelos de manera lineal, añadiendo capas de una en una.\n",
    "- La función `add()` se usa para agregar capas al modelo `Sequential`.\n",
    "- La clase `Dense` se utiliza para crear capas densamente conectadas con diversos parámetros que permiten personalizar la configuración de estas capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(4,), activation='relu'))  # 4 características de entrada\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 clases en salida\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función softmax\n",
    "\n",
    "La función **Softmax** es una función utilizada en aprendizaje automático y, específicamente, en redes neuronales, para transformar un vector de números reales z en un vector de valores probabilísticos en el rango [0, 1] que suman 1. Esta función es comúnmente utilizada en la capa de salida de un clasificador multinomial, donde se desea predecir una de múltiples clases posibles.\n",
    "\n",
    "Dada una entrada z que es un vector de números reales (también conocido como logits), la función Softmax S(z) se define como:\n",
    "\n",
    "$$ S(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}} $$\n",
    "\n",
    "Donde:\n",
    "- $ S(z)_j $ es la salida Softmax para la j-ésima componente del vector z.\n",
    "- K es el número de clases (o longitud del vector z).\n",
    "- $e$ es la base del logaritmo natural (aproximadamente igual a 2.71828).\n",
    "\n",
    "**Intuición:**\n",
    "\n",
    "1. Para cada componente del vector $z$, se toma la función exponencial $e^{z_j}$. Esto asegura que cada componente de la salida está en el rango positivo.\n",
    "\n",
    "2. A continuación, se divide cada $e^{z_j}$ por la suma de todas las exponenciales del vector. Esto normaliza los valores de manera que sumen exactamente 1. \n",
    "\n",
    "El resultado es un vector donde cada componente representa la probabilidad de que la entrada pertenezca a la clase correspondiente. El valor más alto en el vector resultante indica la clase predicha por el modelo.\n",
    "\n",
    "**Usos comunes:**\n",
    "\n",
    "- **Clasificación Multiclase**: Softmax es útil cuando se tiene un problema de clasificación con más de dos clases y las clases son mutuamente excluyentes (es decir, una entrada solo puede pertenecer a una clase).\n",
    "\n",
    "- **Modelos de Lenguaje**: Se usa en modelos de lenguaje basados en redes neuronales para predecir la siguiente palabra en una secuencia, dada una lista de vocabulario.\n",
    "\n",
    "**Diferencia con el Sigmoide**: Mientras que la función sigmoide es adecuada para clasificación binaria y transforma un valor real en un valor entre 0 y 1 (lo que puede interpretarse como una probabilidad), la función Softmax generaliza este concepto a múltiples clases, produciendo un vector de valores que suman 1 y que pueden interpretarse como probabilidades para cada clase posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El conjunto de datos Iris tiene 3 clases, por lo que la función Softmax producirá 3 valores de salida. Vamos a suponer que, después de procesar una flor Iris específica a través de nuestra red neuronal, obtenemos los siguientes logits para las 3 clases:\n",
    "\n",
    "$$z = [0.5, 2.5, -1.5]$$\n",
    "\n",
    "Donde:\n",
    "-  $z_1$ = 0.5  es el logit para la clase \"Iris setosa\".\n",
    "-  $z_2$ = 2.5  es el logit para la clase \"Iris versicolor\".\n",
    "-  $z_3$ = -1.5  es el logit para la clase \"Iris virginica\".\n",
    "\n",
    "Aplicando la función Softmax:\n",
    "\n",
    "$$ S(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{3} e^{z_k}} $$\n",
    "\n",
    "1. Calculamos las exponenciales para cada logit:\n",
    "$$ e^{z_1} = e^{0.5} \\approx 1.65 $$\n",
    "$$ e^{z_2} = e^{2.5} \\approx 12.18 $$\n",
    "$$ e^{z_3} = e^{-1.5} \\approx 0.22 $$\n",
    "\n",
    "2. Calculamos la suma total de estas exponenciales:\n",
    "$$ \\text{Suma} = 1.65 + 12.18 + 0.22 = 14.05 $$\n",
    "\n",
    "3. Aplicamos Softmax:\n",
    "$$ S(z_1) = \\frac{1.65}{14.05} \\approx 0.117 $$\n",
    "$$ S(z_2) = \\frac{12.18}{14.05} \\approx 0.867 $$\n",
    "$$ S(z_3) = \\frac{0.22}{14.05} \\approx 0.016 $$\n",
    "\n",
    "El vector resultante después de aplicar Softmax es:\n",
    "\n",
    "$$ S(z) \\approx [0.117, 0.867, 0.016] $$\n",
    "\n",
    "Esto indica que nuestro modelo estima que hay:\n",
    "- Un 11.7% de probabilidad de que la flor sea \"Iris setosa\".\n",
    "- Un 86.7% de probabilidad de que sea \"Iris versicolor\".\n",
    "- Un 1.6% de probabilidad de que sea \"Iris virginica\".\n",
    "\n",
    "De acuerdo con estas probabilidades, nuestro modelo clasificaría esta flor como \"Iris versicolor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación y entrenamiento:\n",
    "\n",
    "La función `compile()` de un modelo en Keras es fundamental, ya que especifica cómo se entrenará el modelo. Aquí están los parámetros más importantes:\n",
    "\n",
    "1. **optimizer**: Es el algoritmo de optimización que se utilizará para actualizar los pesos del modelo. Los optimizadores son cruciales ya que determinan cómo se reduce el error durante el entrenamiento. Algunos optimizadores populares incluyen:\n",
    "\n",
    "\n",
    "   - `'sgd'`: Stochastic Gradient Descent (Descenso de Gradiente Estocástico). Es una versión básica y frecuentemente utilizada del algoritmo de descenso de gradiente. Y es el que habeis visto en teoría, luego explicare que significa estocástico.\n",
    "\n",
    "   - `'rmsprop'`: Divide la tasa de aprendizaje para un peso por un promedio móvil de las magnitudes de gradientes recientes.\n",
    "\n",
    "    - `'adam'`: Es una variante del método de descenso de gradiente que computa tasas de aprendizaje adaptativas para cada parámetro. Es uno de los optimizadores más recomendados en muchos escenarios (Este seguramente sea vuestro mejor amigo :D)\n",
    "   \n",
    "   También puedes configurar un optimizador con parámetros personalizados:\n",
    "   ```python\n",
    "   from keras.optimizers import SGD\n",
    "   optimizer = SGD(learning_rate=0.001)\n",
    "   ```\n",
    "\n",
    "2. **loss**: Es la función de pérdida (o función objetivo) que el modelo intentará minimizar. Depende del tipo de problema que estés tratando:\n",
    "   \n",
    "   - Problemas de **regresión**:\n",
    "     - `'mean_squared_error'` o `'mse'`: Promedio de los cuadrados de las diferencias entre valores reales y predichos.\n",
    "     - `'mean_absolute_error'` o `'mae'`: Promedio de las diferencias absolutas entre valores reales y predichos.\n",
    "     \n",
    "   - Problemas de **clasificación binaria**:\n",
    "     - `'binary_crossentropy'`: Función de pérdida de entropía cruzada para clasificación binaria.\n",
    "     \n",
    "   - Problemas de **clasificación multiclase**:\n",
    "     - `'categorical_crossentropy'`: Para etiquetas codificadas en one-hot.\n",
    "     - `'sparse_categorical_crossentropy'`: Para etiquetas como enteros.\n",
    "\n",
    "     \n",
    "     \n",
    "3. **metrics**: Una lista de métricas que se evaluarán por el modelo durante el entrenamiento y la prueba. Por ejemplo, en problemas de clasificación, comúnmente usamos `['accuracy']` para evaluar la precisión del modelo. Estas métricas no se utilizan para entrenar el modelo, sino más bien para evaluar su desempeño.\n",
    "\n",
    "   Ejemplo:\n",
    "   ```python\n",
    "   metrics=['accuracy']\n",
    "   ```\n",
    "\n",
    "4. **Nota**: Entraremos mucho más en detalle sobre los optimizadores y funciones de perdida en siguentes seciones, ahora me vale que sepan que existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) # tasa de aprendizaje\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=150, batch_size=10, validation_data=(x_val, y_val))\n",
    "\n",
    "# Puedes utilizar el string 'SGD' en lugar de optimizer pero no podrás modificar la tasa de aprendizaje, por defecto es 0.01\n",
    "# model.compile(optimizer='SGD', loss='MSE', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `fit()` es uno de los métodos más esenciales de los modelos en Keras. Se utiliza para entrenar el modelo durante un número fijo de épocas (iteraciones en un conjunto de datos).\n",
    "\n",
    "A continuación, los parámetros más importantes de `model.fit()`:\n",
    "\n",
    "1. **x**: Datos de entrenamiento. Puede ser un array de NumPy o, en el caso de modelos que tienen múltiples entradas, una lista de arrays de NumPy.\n",
    "\n",
    "2. **y**: Etiquetas o valores objetivo. Al igual que con `x`, puede ser un array de NumPy o una lista de arrays en el caso de modelos multi-salida.\n",
    "\n",
    "3. **batch_size**: Número de muestras por actualización del gradiente. Si no se especifica, el valor predeterminado es 32. \n",
    "\n",
    "4. **epochs**: Número de épocas para entrenar el modelo. Una época es una iteración sobre todo el conjunto de datos `x` y `y`.\n",
    "\n",
    "5. **verbose**: Modo detallado. \n",
    "   - `0`: Silencioso.\n",
    "   - `1`: Muestra una barra de progreso.\n",
    "   - `2`: Muestra una línea por época.\n",
    "\n",
    "7. **validation_split**: Fracción del conjunto de entrenamiento a utilizar como datos de validación. Por ejemplo, un valor de 0.2 utilizará el 20% de los datos como conjunto de validación y el 80% restante como conjunto de entrenamiento.\n",
    "\n",
    "8. **validation_data**: Datos específicos de validación. Es una tupla `(x_val, y_val)` en la que `x_val` son los datos de validación y `y_val` son las etiquetas de esos datos. Esta opción omite el uso de `validation_split`.\n",
    "\n",
    "9. **shuffle**: Si es `True`, baraja las muestras antes de cada época.\n",
    "\n",
    "12. **initial_epoch**: Época en la que empezar el entrenamiento (útil para reanudar un entrenamiento previo).\n",
    "\n",
    "---\n",
    "\n",
    "Un uso típico del método `fit()` para un modelo sería:\n",
    "\n",
    "```python\n",
    "history = model.fit(x=train_data, y=train_labels, batch_size=32, epochs=10, validation_split=0.2, verbose=1)\n",
    "```\n",
    "\n",
    "En este caso, estamos entrenando el modelo con datos `train_data` y etiquetas `train_labels`, usando un tamaño de batch de 32, durante 10 épocas, con el 20% de los datos usados para validación, y mostrando una barra de progreso.\n",
    "\n",
    "La salida (asignada a la variable `history` en el ejemplo) es un objeto que registra las métricas de entrenamiento (y validación, si se proporcionan) para cada época. Esto es útil para análisis y visualización después del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `batch_size` es fundamental para entender cómo se entrena un modelo de aprendizaje profundo, así que vamos a desglosarlo:\n",
    "\n",
    "### **`batch_size` en el contexto del Aprendizaje Profundo**\n",
    "\n",
    "El `batch_size` se refiere al número de ejemplos de entrenamiento utilizados en una iteración (o paso) para actualizar los pesos del modelo.\n",
    "\n",
    "### **Entrenamiento por Lotes, Estocástico y Mini-Lote**\n",
    "\n",
    "La elección del `batch_size` nos lleva a tres modos distintos de entrenamiento:\n",
    "\n",
    "1. **Entrenamiento por Lotes (Batch Gradient Descent)**:\n",
    "    - `batch_size` = tamaño del conjunto de entrenamiento (todos los datos)\n",
    "    - Los pesos se actualizan una vez por época después de haber visto todos los datos.\n",
    "    - Es determinista, es decir, para los mismos datos de entrada y el mismo modelo inicial, siempre obtendrás el mismo resultado.\n",
    "    - Puede ser computacionalmente ineficiente para conjuntos de datos grandes, ya que requiere que todo el conjunto de datos esté en memoria.\n",
    "\n",
    "2. **Entrenamiento Estocástico (Stochastic Gradient Descent, SGD)**:\n",
    "    - `batch_size` = 1\n",
    "    - Los pesos se actualizan después de ver cada dato individualmente.\n",
    "    - Introduce mucho ruido en la actualización de los pesos, lo que puede ayudar a escapar de óptimos locales, pero también puede hacer que el entrenamiento sea más inestable.\n",
    "    - Generalmente, requiere más épocas para converger en comparación con el entrenamiento por lotes.\n",
    "\n",
    "3. **Entrenamiento con Mini-Lote (Mini-Batch Gradient Descent)**:\n",
    "    - `1` < `batch_size` < tamaño del conjunto de entrenamiento\n",
    "    - Los pesos se actualizan después de ver un subconjunto (mini-lote) de datos.\n",
    "    - Combina lo mejor de los dos mundos anteriores: es computacionalmente más eficiente que SGD y puede beneficiarse del ruido en las actualizaciones de pesos para escapar de óptimos locales.\n",
    "    - Es el método más comúnmente utilizado en la práctica.\n",
    "\n",
    "### **Impacto de `batch_size` en el entrenamiento**\n",
    "\n",
    "- **Velocidad de entrenamiento**: Un `batch_size` más grande puede procesar los datos más rápidamente, ya que se beneficia de las optimizaciones de hardware, en particular en GPUs. Sin embargo, un `batch_size` demasiado grande puede exceder la memoria del hardware.\n",
    "\n",
    "- **Convergencia**: Un `batch_size` más pequeño puede introducir suficiente ruido para evitar óptimos locales, pero también puede llevar a una convergencia inestable. Un tamaño de lote más grande proporciona una estimación más precisa del gradiente, pero con menos actualizaciones por época.\n",
    "\n",
    "- **Calidad del modelo**: No hay un tamaño de lote óptimo que funcione para todos los problemas. Es un hiperparámetro que, en muchos casos, debe ajustarse experimentalmente. En algunos problemas, los modelos entrenados con mini-lotes más pequeños generalizan mejor en datos no vistos, mientras que en otros casos podría ser lo contrario.\n",
    "\n",
    "### **Consejo práctico**\n",
    "\n",
    "Cuando no estés seguro de qué `batch_size` usar, los valores comunes que suelen probarse son 32, 64, 128, 256. Sin embargo, siempre depende de la memoria disponible (especialmente si estás utilizando GPUs) y del problema específico que estés abordando. Es útil experimentar y validar el rendimiento del modelo con diferentes tamaños de lote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "\n",
    "plt.title('Entrenamiento IRIS')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print validation accuracy\n",
    "plt.plot(history.history['val_loss'], label='Loss')\n",
    "plt.plot(history.history['val_accuracy'], label='accuracy')\n",
    "\n",
    "plt.title('Validación IRIS')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar un modelo en Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TO_SAVE_MODELS = 'models/'\n",
    "NAME_FILE_IRIS_MODEL = 'iris_model.h5'\n",
    "path = os.path.join(FOLDER_TO_SAVE_MODELS, NAME_FILE_IRIS_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar un modelo entrenado en Keras es sencillo y puede hacerse de varias formas, dependiendo de lo que desees conservar (estructura del modelo, pesos del modelo, configuración de entrenamiento, etc.). Aquí te mostraré dos métodos principales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar un modelo entrenado en Keras es sencillo y puede hacerse de varias formas, dependiendo de lo que desees conservar (estructura del modelo, pesos del modelo, configuración de entrenamiento, etc.). Aquí te mostraré dos métodos principales:\n",
    "\n",
    "### 1. Guardar todo el modelo (estructura + pesos + configuración de entrenamiento):\n",
    "\n",
    "Puedes guardar el modelo completo en un solo archivo. Esto incluirá:\n",
    "- La arquitectura del modelo\n",
    "- Los pesos del modelo\n",
    "- La configuración de entrenamiento (lo que especificaste al compilar el modelo)\n",
    "- El optimizador y su estado (si deseas reanudar el entrenamiento donde lo dejaste)\n",
    "\n",
    "Es decir que es un copia identitica del experiemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "model.save(path)\n",
    "\n",
    "# Cargar el modelo\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model(path)\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Guardar solo la arquitectura o solo los pesos:\n",
    "\n",
    "a. **Guardar la arquitectura**:\n",
    "\n",
    "Puedes guardar la estructura del modelo (sin ningún peso) en formato JSON. Esto es útil sobre todo para temas de documentación o para replicar modelos en otros proyectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en formato JSON\n",
    "json_string = model.to_json()\n",
    "\n",
    "import json\n",
    "print(json.dumps(json.loads(json_string), indent=2))\n",
    "\n",
    "# Para cargar el modelo desde JSON\n",
    "from keras.models import model_from_json\n",
    "loaded_model = model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. **Guardar solo los pesos**:\n",
    "\n",
    "Puedes guardar solo los pesos del modelo, esto es útil cuando veamos transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los pesos del modelo\n",
    "MODEL_WEIGHTS = 'iris_model_weights.h5'\n",
    "path_to_save_weigths = os.path.join(FOLDER_TO_SAVE_MODELS, MODEL_WEIGHTS)\n",
    "\n",
    "model.save_weights(path_to_save_weigths)\n",
    "\n",
    "# Cargar los pesos en un modelo con la misma arquitectura\n",
    "model.load_weights(path_to_save_weigths)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos métodos te permiten guardar y cargar modelos en Keras de manera efectiva. Dependiendo de tus necesidades (por ejemplo, si deseas reutilizar solo la arquitectura del modelo en otro proyecto, o si necesitas guardar todo para reanudar el entrenamiento más tarde), puedes elegir el método que más te convenga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producción\n",
    "Una vez entrenada y testeada la red, podemos ponerla en producción. Si queremos hacer una clasifiación invocaremos el método predict del modelo.\n",
    "\n",
    "Vamos a ver qué resultados nos ofrece la red si introducimos el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pass y_test to a numpy numerical array\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "for p, l in zip(predictions, y_test):\n",
    "    print(p, \"->\", l)\n",
    "    if np.argmax(p) == np.argmax(l):\n",
    "        print(p, \"->\", l)\n",
    "    else:\n",
    "        print(p, \"->\", l, \"✘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning para Regression\n",
    "\n",
    "Para la regresión lo único que necesitamos es modificar la capa de salida, en este caso, en lugar de una capa con una neurona y una función de activación, usaremos una capa con una neurona y sin función de activación. Esto es porque queremos que la salida sea un valor continuo y no una probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Dense\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "# Normalizar datos\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "# Crear modelo\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='SGD', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(train_data, train_targets, epochs=80, batch_size=16, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "print(f'MAE: {test_mae_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.plot(history.history['mae'], label='MAE')\n",
    "plt.plot(history.history['val_mae'], label='val MAE')\n",
    "\n",
    "plt.title('Entrenamiento Boston Housing')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visión por computador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una red neuronal para tratar ahora con un conjunto mucho más grande, el dataset MNIST. Verás que, a medida que tratamos con conjuntos mayores, el tiempo de procesamiento se incrementa y la necesidad de contar con una GPU crece al mismo ritmo. Échale un vistazo a este [vídeo](https://www.youtube.com/watch?v=qcOjR-sJkUY&ab_channel=CayetanoGuerra) para que te vayas familiarizando con Google Colab, por si necesitas usarlo.\n",
    "\n",
    "[Aquí](https://www.adictosaltrabajo.com/2019/06/04/google-colab-python-y-machine-learning-en-la-nube/) tienes un buen tutorial sobre Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto MNIST\n",
    "El [conjunto MNIST](https://en.wikipedia.org/wiki/MNIST_database) está formado por 70.000 imágenes de dígitos manuscritos del 0 al 9 con un tamaño de 28x28 en escala de grises. A su vez, el conjunto se divide en 60.000 imágenes para entrenamiento y 10.000 para test.\n",
    "\n",
    "Vamos a utilizar este *dataset* para entrenar una red y ver qué precisión obtenemos al clasificar el conjunto de test. Piensa bien lo que pretendemos lograr: **hacer una red que será capaz de “ver”**, aunque, por ahora, solo sean imágenes de dos dimensiones.\n",
    "\n",
    "El conjunto MNIST es muy popular y se utiliza mucho para aprender y probar redes, así que Keras ya lo incluye como parte de la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Veamos la forma tiene x_train\n",
    "print(\"Shape:\", train_images.shape)  # 60.000 imágenes de 28x28\n",
    "\n",
    "# Veamos una imagen cualquiera, por ejemplo, con el índice 125\n",
    "image = np.array(train_images[125], dtype='float')\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"Label:\", train_labels[125])\n",
    "print(\"Class:\\n\", np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es necesario saber en qué rango de valores se mueven nuestras muestras. Vemos que cada pixel es un byte con un rango de valores que va desde el 0 hasta el 255 en formato entero. Esta escala no es muy adecuada para la red. Podemos facilitar mucho el trabajo de entrenamiento si transformamos esta escala en otra centrada en el 0 y con un rango de valores entre -0.5 y 0.5. Y, por supuesto, en formato real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar imágenes\n",
    "print(\"Max value:\", max(train_images[125].reshape(784)))\n",
    "print(\"Min value:\", min(train_images[125].reshape(784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Cargar datos\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalizar imágenes\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Convertir etiquetas a formato categórico\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Crear modelo\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='MSE',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flatten**: Esta capa se encarga de convertir la imagen de 28x28 en un vector de 784 elementos. Es decir, aplana la imagen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', np.round(test_acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "\n",
    "plt.title('Entrenamiento MNIST')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Experimenta con los hiperparametros de la red para los tres conjuntos de datos\n",
    "    - Número de capas\n",
    "    - Número de neuronas por capa\n",
    "    - Funciones de activación\n",
    "    - Optimizador\n",
    "    - Función de pérdida\n",
    "    - Número de épocas\n",
    "    - Tamaño del batch\n",
    "    - etc.\n",
    "2. Anota los resultados obtenidos en una tabla, para cada conjunto de datos, con los hiperparametros que mejor resultado te han dado.\n",
    "3. ¿Qué conjunto de datos es más dificil de clasificar? ¿Por qué?\n",
    "4. ¿Qué configuraciones dieron el mejor rendimiento y por qué crees que sucedió?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- [Keras](https://keras.io/)\n",
    "- [Keras Tutorial: Deep Learning in Python](https://www.datacamp.com/community/tutorials/deep-learning-python)\n",
    "- [Keras Tutorial: How to get started with Keras, Deep Learning, and Python](https://www.pyimagesearch.com/2016/07/18/installing-keras-for-deep-learning/)\n",
    "- [Redes neuronales 4](https://nbviewer.org/url/cayetanoguerra.github.io/ia/nbpy/redneuronal4.ipynb)\n",
    "- [Redes neuronales 5](https://nbviewer.org/url/cayetanoguerra.github.io/ia/nbpy/redneuronal5.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
